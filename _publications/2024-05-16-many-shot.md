---
title: "Many-Shot In-Context Learning in Multimodal Foundation Models"
collection: publications
category: manuscripts
permalink: /publication/2024-05-16-many-shot
excerpt: 'In-Context Learning, Multimodal Foundation Models, Adapting Foundation Models'
date: 2024-05-16
venue: '1st In-Context Learning Workshop, International Conference on Machine Learning (ICML), 2024'
slidesurl: 
paperurl: 'http://ahmed14974.github.io/files/manyshot.pdf'
citation: 
---
<!-- Your Name, You. (2010). &quot;Paper Title Number 2.&quot; <i>Journal 1</i>. 1(2). -->

<img width="851" alt="image" src="https://github.com/user-attachments/assets/3227f137-fbcb-46ce-98fe-a8e3cf76c198">

Large language models are effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have enabled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 14 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (image classification, visual QA, and object localization). We observe that many-shot ICL, including up to almost 2,000 demonstrating examples, leads to substantial improvements compared to few-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly up to the maximum number of tested examples on many datasets. We also find open-weights multimodal foundation models like Llama 3.2-Vision do not benefit from the demonstrating examples, highlighting an important gap between open and closed multimodal foundation models. Given the high inference costs required for many-shot ICL, we also explore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and many-shot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro learns more quickly than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains. Our codebase is publicly available at this [link](https://github.com/stanfordmlgroup/ManyICL).
